{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as  optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "mnist_train = datasets.MNIST(root='data/',\n",
    "                          train=True,\n",
    "                          transform=transforms.ToTensor(),\n",
    "                          download=True)\n",
    "\n",
    "mnist_test = datasets.MNIST(root='data/',\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor(),\n",
    "                         download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(mnist_train, \n",
    "                         batch_size=4,\n",
    "                         shuffle=True, \n",
    "                         drop_last=False)\n",
    "\n",
    "testloader = DataLoader(mnist_test, \n",
    "                        batch_size=4,\n",
    "                        shuffle=False,\n",
    "                        drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "torch.Size([4, 1, 28, 28])\n",
      "tensor([9, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "#iterator, 데이터를 하나씩 돌면서 탐색한다.\n",
    "trainiter = iter(trainloader)\n",
    "images, labels = trainiter.next()\n",
    "\n",
    "print(len(trainloader))\n",
    "print(images.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "print(len(testloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer \n",
    "src = {'input_size':28*28,\n",
    "       'hidden_size1':256,\n",
    "       'hidden_size2':158,\n",
    "       'output_size':10,\n",
    "       'num_epochs':2,\n",
    "       'batch_size':100,\n",
    "       'learning_rate':0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# nn에 필요한 다양한 함수들을 제공한다.\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TobigsNet(nn.Module):\n",
    "    def __init__(self, src):\n",
    "        super(TobigsNet, self).__init__()   \n",
    "        # two layer\n",
    "        self.fc1 = nn.Linear(src['input_size'], src['hidden_size1'])\n",
    "        self.fc2 = nn.Linear(src['hidden_size1'], src['hidden_size2'])\n",
    "        self.fc3 = nn.Linear(src['hidden_size2'], src['output_size'])\n",
    "    \n",
    "    def forward(self, img):\n",
    "        # 차원을 바꾼다.\n",
    "        x = img.view(img.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        y = F.softmax(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "model = TobigsNet(src)\n",
    "# oupput \n",
    "y = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss corssentropy\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# descent gradient \n",
    "# 미니배치 하강법.\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), \n",
    "                      src['learning_rate'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\Users\\sora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1000/15000], Loss: 2.3016\n",
      "Epoch [1/2], Step [2000/15000], Loss: 2.2982\n",
      "Epoch [1/2], Step [3000/15000], Loss: 2.2910\n",
      "Epoch [1/2], Step [4000/15000], Loss: 2.2638\n",
      "Epoch [1/2], Step [5000/15000], Loss: 2.1344\n",
      "Epoch [1/2], Step [6000/15000], Loss: 1.9804\n",
      "Epoch [1/2], Step [7000/15000], Loss: 1.8708\n",
      "Epoch [1/2], Step [8000/15000], Loss: 1.8260\n",
      "Epoch [1/2], Step [9000/15000], Loss: 1.7854\n",
      "Epoch [1/2], Step [10000/15000], Loss: 1.7639\n",
      "Epoch [1/2], Step [11000/15000], Loss: 1.7447\n",
      "Epoch [1/2], Step [12000/15000], Loss: 1.7419\n",
      "Epoch [1/2], Step [13000/15000], Loss: 1.7275\n",
      "Epoch [1/2], Step [14000/15000], Loss: 1.7200\n",
      "Epoch [1/2], Step [15000/15000], Loss: 1.7383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:46<00:46, 46.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [1000/15000], Loss: 1.7149\n",
      "Epoch [2/2], Step [2000/15000], Loss: 1.7095\n",
      "Epoch [2/2], Step [3000/15000], Loss: 1.7195\n",
      "Epoch [2/2], Step [4000/15000], Loss: 1.7213\n",
      "Epoch [2/2], Step [5000/15000], Loss: 1.7073\n",
      "Epoch [2/2], Step [6000/15000], Loss: 1.7120\n",
      "Epoch [2/2], Step [7000/15000], Loss: 1.7174\n",
      "Epoch [2/2], Step [8000/15000], Loss: 1.7227\n",
      "Epoch [2/2], Step [9000/15000], Loss: 1.7038\n",
      "Epoch [2/2], Step [10000/15000], Loss: 1.6909\n",
      "Epoch [2/2], Step [11000/15000], Loss: 1.7090\n",
      "Epoch [2/2], Step [12000/15000], Loss: 1.7089\n",
      "Epoch [2/2], Step [13000/15000], Loss: 1.7031\n",
      "Epoch [2/2], Step [14000/15000], Loss: 1.7047\n",
      "Epoch [2/2], Step [15000/15000], Loss: 1.7056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [01:33<00:00, 46.48s/it]\n"
     ]
    }
   ],
   "source": [
    "#진행 표시바\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(src['num_epochs'])):\n",
    "    #epoch 마다 loss 갱신해야 함으로, current loss =0으로 지정!\n",
    "    current_loss = 0.0\n",
    "#     model.train(True)\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # loss를 최소화 하는 방향으로, backward gradient descent weight 업데이트!\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        step = i + 1\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "        if step % 1000 == 0 and step != 0:     # print every 1000 mini-batches\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %\n",
    "                  #  평균 loss를 사용해야 한다.\n",
    "                  (epoch + 1, src['num_epochs'], step, len(trainloader)//1000 * 1000, current_loss / 1000))\n",
    "            current_loss = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 2500 test images: 75 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "# test set을 돈다!\n",
    "for i, data in enumerate(testloader):\n",
    "    inputs, labels = data\n",
    "#     images = images.view(-1, 28*28)\n",
    "    outputs = model(inputs)\n",
    "    # outputs 확률에서 가장 크게 나온 값을 가져온다!\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.shape[0]\n",
    "    # 만약 두 값이 같으면 sum에 +1 ~!\n",
    "    \n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "# 몇 %를 맞췄는지.\n",
    "print('Accuracy of the network on the 2500 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam 을 이용하여 optimalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch norm & dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TobigsNet_nb(nn.Module):\n",
    "    def __init__(self, src):\n",
    "        super(TobigsNet_nb, self).__init__()   \n",
    "        \n",
    "        self.linear1 = nn.Linear(src['input_size'], src['hidden_size1'])\n",
    "        self.linear2 = nn.Linear(src['hidden_size1'], src['hidden_size2'])\n",
    "        self.linear3 = nn.Linear(src['hidden_size2'], src['output_size'])\n",
    "        self.bn1 = nn.BatchNorm1d(src['hidden_size1'])\n",
    "        self.bn2 = nn.BatchNorm1d(src['hidden_size2'])\n",
    "\n",
    "    \n",
    "    def forward(self, img):\n",
    "        # 차원을 바꾼다.\n",
    "        x = img.view(img.shape[0], -1)\n",
    "        x = self.bn1(F.relu(self.linear1(x)))\n",
    "        x = self.bn2(F.relu(self.linear2(x)))\n",
    "        x = F.dropout(x)\n",
    "        x = self.linear3(x)\n",
    "        y = F.softmax(x)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1372, 0.1803, 0.0746, 0.0485, 0.0639, 0.0815, 0.0639, 0.1416, 0.1423,\n",
      "         0.0661],\n",
      "        [0.1269, 0.0602, 0.0694, 0.1179, 0.1002, 0.0717, 0.1432, 0.1606, 0.0619,\n",
      "         0.0880],\n",
      "        [0.0770, 0.0635, 0.1556, 0.0581, 0.2759, 0.0949, 0.0798, 0.0943, 0.0580,\n",
      "         0.0430],\n",
      "        [0.0990, 0.2020, 0.0572, 0.0643, 0.0677, 0.0883, 0.0708, 0.0705, 0.1846,\n",
      "         0.0954]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "model2 = TobigsNet_nb(src)\n",
    "# oupput \n",
    "y = model2(images)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#adam 적용 \n",
    "optimizer_a = optim.Adam(model2.parameters(), \n",
    "                      src['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]C:\\Users\\sora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [1000/15000], Loss: 1.9406\n",
      "Epoch [1/2], Step [2000/15000], Loss: 1.8624\n",
      "Epoch [1/2], Step [3000/15000], Loss: 1.8663\n",
      "Epoch [1/2], Step [4000/15000], Loss: 1.8566\n",
      "Epoch [1/2], Step [5000/15000], Loss: 1.8447\n",
      "Epoch [1/2], Step [6000/15000], Loss: 1.8519\n",
      "Epoch [1/2], Step [7000/15000], Loss: 1.8401\n",
      "Epoch [1/2], Step [8000/15000], Loss: 1.8299\n",
      "Epoch [1/2], Step [9000/15000], Loss: 1.8334\n",
      "Epoch [1/2], Step [10000/15000], Loss: 1.8285\n",
      "Epoch [1/2], Step [11000/15000], Loss: 1.8270\n",
      "Epoch [1/2], Step [12000/15000], Loss: 1.8149\n",
      "Epoch [1/2], Step [13000/15000], Loss: 1.8077\n",
      "Epoch [1/2], Step [14000/15000], Loss: 1.8115\n",
      "Epoch [1/2], Step [15000/15000], Loss: 1.8214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 1/2 [04:26<04:26, 266.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/2], Step [1000/15000], Loss: 1.8150\n",
      "Epoch [2/2], Step [2000/15000], Loss: 1.8185\n",
      "Epoch [2/2], Step [3000/15000], Loss: 1.8282\n",
      "Epoch [2/2], Step [4000/15000], Loss: 1.8181\n",
      "Epoch [2/2], Step [5000/15000], Loss: 1.8227\n",
      "Epoch [2/2], Step [6000/15000], Loss: 1.8313\n",
      "Epoch [2/2], Step [7000/15000], Loss: 1.8160\n",
      "Epoch [2/2], Step [8000/15000], Loss: 1.7964\n",
      "Epoch [2/2], Step [9000/15000], Loss: 1.8177\n",
      "Epoch [2/2], Step [10000/15000], Loss: 1.8173\n",
      "Epoch [2/2], Step [11000/15000], Loss: 1.8119\n",
      "Epoch [2/2], Step [12000/15000], Loss: 1.8184\n",
      "Epoch [2/2], Step [13000/15000], Loss: 1.8146\n",
      "Epoch [2/2], Step [14000/15000], Loss: 1.8198\n",
      "Epoch [2/2], Step [15000/15000], Loss: 1.8316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 2/2 [09:36<00:00, 279.56s/it]\n"
     ]
    }
   ],
   "source": [
    "#진행 표시바\n",
    "from tqdm import tqdm\n",
    "\n",
    "for epoch in tqdm(range(src['num_epochs'])):\n",
    "    current_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, labels = data\n",
    "        optimizer_a.zero_grad()\n",
    "        \n",
    "        outputs = model2(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_a.step()\n",
    "\n",
    "        step = i + 1\n",
    "        current_loss += loss.item()\n",
    "        \n",
    "        if step % 1000 == 0 and step != 0:     # print every 1000 mini-batches\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' %\n",
    "                  #  평균 loss를 사용해야 한다.\n",
    "                  (epoch + 1, src['num_epochs'], step, len(trainloader)//1000 * 1000, current_loss / 1000))\n",
    "            current_loss = 0.0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sora\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 2500 test images: 66 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "# test set을 돈다!\n",
    "for i, data in enumerate(testloader):\n",
    "    inputs, labels = data\n",
    "    outputs = model2(inputs)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.shape[0]\n",
    "    \n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 2500 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 흠..왜 정확도가 더 낮아졌을까...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
